{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 1 T-test\n",
    "\n",
    "In statistics, t-test is used to test if two data samples have a significant difference between their means. There are two types of t-test:\n",
    "\n",
    "* **Student's t-test** (a.k.a. independent or uncorrelated t-test). This type of t-test is to compare the samples of **two independent populations** (e.g. test scores of students in two different classes). `scipy` provides the [`ttest_ind`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_ind.html) method to conduct a student's t-test.\n",
    "\n",
    "* **Paired t-test** (a.k.a. dependent or correlated t-test). This type of t-test is to compare the samples of **the same population** (e.g. scores of different tests of students in the same class). `scipy` provides the [`ttest_rel`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_rel.html) method to conduct paired t-test.\n",
    "\n",
    "Both types of t-tests return a number which is called the **p-value**. If p-value is below 0.05, we can confidently declare the null-hypothesis is rejected and the difference is significant. If p-value is between 0.05 and 0.1, we may also declare the null-hypothesis is rejected but we are not highly confident. If p-value is above 0.1 we do not reject the null-hypothesis.\n",
    "\n",
    "Read more about the t-test in [this article](http://b.link/test50) and [this Quora](http://b.link/unpaired97). Make sure you understand when to use which type of t-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of this challenge is to test whether different groups of pokemon (e.g. Legendary vs Normal, Generation 1 vs 2, single-type vs dual-type) have different stats (e.g. HP, Attack, Defense, etc).\n",
    "- Import `pokemon.csv` dataset and store called pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv('data/pokemon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we want to define a function with which we can test the means of a feature set of two samples. \n",
    "\n",
    "In the next cell you'll see the annotations of the Python function that explains what this function does and its arguments and returned value. This type of annotation is called **docstring** which is a convention used among Python developers. The docstring convention allows developers to write consistent tech documentation for their code so that others can read. It also allows some websites to automatically parse the docstrings and display user-friendly documentation.\n",
    "\n",
    "Follow the specifications of the docstring and complete the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a two-sided test for the null hypothesis that 2 independent \n",
    "# samples have identical average (expected) values. This test assumes \n",
    "# that the populations have identical variances by default.\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_features(s1, s2, features=['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Total']):\n",
    "    \"\"\"Test means of a feature set of two samples\n",
    "    \n",
    "    Args:\n",
    "        s1 (dataframe): sample 1\n",
    "        s2 (dataframe): sample 2\n",
    "        features (list): an array of features to test\n",
    "    \n",
    "    Returns:\n",
    "        dict: a dictionary of t-test scores for each feature where the feature name is the key and the p-value is the value\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        results[feature] = ttest_ind(s1[feature], s2[feature], equal_var = False).pvalue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the `t_test_features` function, conduct a t-test for Legendary vs non-Legendary pokemons.\n",
    "\n",
    "*Hint: your output should look like below:*\n",
    "\n",
    "```\n",
    "{'HP': 1.0026911708035284e-13,\n",
    " 'Attack': 2.520372449236646e-16,\n",
    " 'Defense': 4.8269984949193316e-11,\n",
    " 'Sp. Atk': 1.5514614112239812e-21,\n",
    " 'Sp. Def': 2.2949327864052826e-15,\n",
    " 'Speed': 1.049016311882451e-18,\n",
    " 'Total': 9.357954335957446e-47}\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "alpha = 0.05\n",
    "\n",
    "# Find legendary and normal pokemon\n",
    "legendary = pokemon[pokemon['Legendary'] == True]\n",
    "normal = pokemon[pokemon['Legendary'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HP': 1.0026911708035284e-13,\n",
       " 'Attack': 2.520372449236646e-16,\n",
       " 'Defense': 4.8269984949193316e-11,\n",
       " 'Sp. Atk': 1.5514614112239812e-21,\n",
       " 'Sp. Def': 2.2949327864052826e-15,\n",
       " 'Speed': 1.049016311882451e-18,\n",
       " 'Total': 9.357954335957446e-47}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-test Features Function\n",
    "## Ho: legendary pokemon have identical average values to normal pokemon\n",
    "## Ha: legendary pokemon have different average values to normal pokemon\n",
    "test_results = t_test_features(legendary, normal)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->  HP\n",
      "Null hypothesis rejected\n",
      "->  Attack\n",
      "Null hypothesis rejected\n",
      "->  Defense\n",
      "Null hypothesis rejected\n",
      "->  Sp. Atk\n",
      "Null hypothesis rejected\n",
      "->  Sp. Def\n",
      "Null hypothesis rejected\n",
      "->  Speed\n",
      "Null hypothesis rejected\n",
      "->  Total\n",
      "Null hypothesis rejected\n"
     ]
    }
   ],
   "source": [
    "# Can we reject the null hypothesis?\n",
    "for feature in test_results:\n",
    "    p = test_results[feature]\n",
    "    print('-> ', feature)\n",
    "    print(\"Null hypothesis rejected\") if p < alpha else print(\"Null hypothesis can't be rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the test results above, what conclusion can you make? Do Legendary and non-Legendary pokemons have significantly different stats on each feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legendary and normal pokemon have significantly different means on each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, conduct a t-test for Generation 1 and Generation 2 pokemons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "alpha = 0.05\n",
    "\n",
    "# Find generation 1 and generation 2 pokemon\n",
    "gen_one = pokemon[pokemon['Generation'] == 1]\n",
    "gen_two = pokemon[pokemon['Generation'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HP': 0.14551697834219623,\n",
       " 'Attack': 0.24721958967217725,\n",
       " 'Defense': 0.5677711011725426,\n",
       " 'Sp. Atk': 0.12332165977104394,\n",
       " 'Sp. Def': 0.18829872292645752,\n",
       " 'Speed': 0.00239265937312135,\n",
       " 'Total': 0.5631377907941676}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-test Features Function\n",
    "## Ho: generation 1 pokemon have identical average values to generation 2 pokemon\n",
    "## Ha: generation 1 pokemon have different average values to generation 2 pokemon\n",
    "test_results = t_test_features(gen_one, gen_two)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->  HP\n",
      "Null hypothesis can't be rejected\n",
      "->  Attack\n",
      "Null hypothesis can't be rejected\n",
      "->  Defense\n",
      "Null hypothesis can't be rejected\n",
      "->  Sp. Atk\n",
      "Null hypothesis can't be rejected\n",
      "->  Sp. Def\n",
      "Null hypothesis can't be rejected\n",
      "->  Speed\n",
      "Null hypothesis rejected\n",
      "->  Total\n",
      "Null hypothesis can't be rejected\n"
     ]
    }
   ],
   "source": [
    "# Can we reject the null hypothesis?\n",
    "for feature in test_results:\n",
    "    p = test_results[feature]\n",
    "    print('-> ', feature)\n",
    "    print(\"Null hypothesis rejected\") if p < alpha else print(\"Null hypothesis can't be rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What conclusions can you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't say that generation 1 and generation 2 pokemon have significantly different means on each feature, except for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare pokemons who have a single type vs those having two types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "alpha = 0.05\n",
    "# Find single-type and two-type pokemon\n",
    "one_type = pokemon[pokemon['Type 2'].isnull()]\n",
    "two_type = pokemon[pokemon['Type 2'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HP': 0.11314389855379421,\n",
       " 'Attack': 0.00014932578145948305,\n",
       " 'Defense': 2.7978540411514693e-08,\n",
       " 'Sp. Atk': 0.00013876216585667842,\n",
       " 'Sp. Def': 0.00010730610934512779,\n",
       " 'Speed': 0.02421703281819094,\n",
       " 'Total': 1.1157056505229961e-07}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-test Features Function\n",
    "## Ho: single-type pokemon have identical average values to two-type 2 pokemon\n",
    "## Ha: single-type pokemon have different average values to two-type 2 pokemon\n",
    "test_results = t_test_features(one_type, two_type)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->  HP\n",
      "Null hypothesis can't be rejected\n",
      "->  Attack\n",
      "Null hypothesis rejected\n",
      "->  Defense\n",
      "Null hypothesis rejected\n",
      "->  Sp. Atk\n",
      "Null hypothesis rejected\n",
      "->  Sp. Def\n",
      "Null hypothesis rejected\n",
      "->  Speed\n",
      "Null hypothesis rejected\n",
      "->  Total\n",
      "Null hypothesis rejected\n"
     ]
    }
   ],
   "source": [
    "# Can we reject the null hypothesis?\n",
    "for feature in test_results:\n",
    "    p = test_results[feature]\n",
    "    print('-> ', feature)\n",
    "    print(\"Null hypothesis rejected\") if p < alpha else print(\"Null hypothesis can't be rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What conclusions can you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-type and two-type pokemon have significantly different means on each feature, \n",
    "# except for HP, for which we can't reject that they have equal means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we want to compare whether there are significant differences between  `Attack` vs `Defense`  and  `Sp. Atk` vs `Sp. Def` of all pokemons. Please write your code below.\n",
    "\n",
    "*Hint: are you comparing different populations or the same population?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are comparing the same population, so we are going to use the ttest_rel function.\n",
    "# This is a two-sided test for the null hypothesis that 2 related or repeated samples \n",
    "# have identical average (expected) values.\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on two related samples\n",
    "## Ho: pokemon's defense has identical average values to pokemon's attack\n",
    "## Ha: pokemon's defense has different average values to pokemon's attack\n",
    "att_def_p = ttest_rel(pokemon['Attack'], pokemon['Defense']).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ho: pokemon's special defense has identical average values to pokemon's special attack\n",
    "## Ha: pokemon's special defense has different average values to pokemon's special attack\n",
    "sp_att_def_p = ttest_rel(pokemon['Sp. Atk'], pokemon['Sp. Def']).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Attack - Defense': 1.7140303479358558e-05,\n",
       " 'Special Attack - Special Defense': 0.3933685997548122}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = {'Attack - Defense': att_def_p, 'Special Attack - Special Defense': sp_att_def_p}\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->  Attack - Defense\n",
      "Null hypothesis rejected\n",
      "->  Special Attack - Special Defense\n",
      "Null hypothesis can't be rejected\n"
     ]
    }
   ],
   "source": [
    "# Can we reject the null hypothesis?\n",
    "for features in test_results:\n",
    "    p = test_results[features]\n",
    "    print('-> ', features)\n",
    "    print(\"Null hypothesis rejected\") if p < alpha else print(\"Null hypothesis can't be rejected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What conclusions can you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokemon's attack and defense have significantly different means. However, we \n",
    "# can't reject that special attack and special defense have equal means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 2 - ANOVA\n",
    "\n",
    "In statistics, **Analysis of Variance (ANOVA)** is also used to analyze the differences among group means. The difference between t-test and ANOVA is the former is used to compare two groups whereas the latter is used to compare three or more groups. [Read more about the difference between t-test and ANOVA](https://keydifferences.com/difference-between-t-test-and-anova.html).\n",
    "\n",
    "From the ANOVA test, you receive two numbers. The first number is called the **F-value** which indicates whether your null-hypothesis can be rejected. The critical F-value that rejects the null-hypothesis varies according to the number of total subjects and the number of subject groups in your experiment. In [this table](https://www.itl.nist.gov/div898/handbook/eda/section3/eda3673.htm) you can find the critical values of the F distribution. **If you are confused by the massive F-distribution table, don't worry. Skip F-value for now and study it at a later time. In this challenge you only need to look at the p-value.**\n",
    "\n",
    "The p-value is another number yielded by ANOVA which already takes the number of total subjects and the number of experiment groups into consideration. **Typically if your p-value is less than 0.05, you can declare the null-hypothesis is rejected.**\n",
    "\n",
    "In this challenge, we want to understand whether there are significant differences among various types of pokemons' `Total` value, i.e. Grass vs Poison vs Fire vs Dragon... There are many types of pokemons which makes it a perfect use case for ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve our goal, we use three steps:\n",
    "- Extract the unique values of the pokemon types.\n",
    "- Select dataframes for each unique pokemon type.\n",
    "- Conduct ANOVA analysis across the pokemon types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's obtain the unique values of the pokemon types. These values should be extracted from Type 1 and Type 2 aggregated. Assign the unique values to a variable called `unique_types`.\n",
    "\n",
    "*Hint: the correct number of unique types is 19 including `NaN`. You can disregard `NaN` in the next step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique types\n",
    "unique_types = pokemon['Type 1'].append(pokemon['Type 2']).unique()\n",
    "\n",
    "# Check number of types\n",
    "len(unique_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, we will create a list named `pokemon_totals` to contain the `Total` values of each unique type of pokemons.\n",
    "\n",
    "Why we do use a list instead of a dictionary to store the pokemon `Total`? It's because ANOVA only tells us whether there is a significant difference of the group means but does not tell which group(s) are significantly different. Therefore, we don't need to know which `Total` belongs to which pokemon type.\n",
    "- Loop through `unique_types` and append the selected type's `Total` to `pokemon_groups`.\n",
    "- Skip the `NaN` value in `unique_types`. `NaN` is a `float` variable which you can find out by using `type()`. The valid pokemon type values are all of the `str` type.\n",
    "- At the end, the length of your `pokemon_totals` should be 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you read the pokemon data using pandas.read_csv, take into account what the hint says \n",
    "# and use \"if isinstance(ptype, str)\" as a condition to skip NaN values. \n",
    "# If you read the pokemon data using Ironhack's database, NaN values are empty strings, so \n",
    "# use \"if ptype != ''\" to skip empty values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty list\n",
    "pokemon_totals = []\n",
    "\n",
    "# Loop unique types\n",
    "for ptype in unique_types:\n",
    "    if isinstance(ptype, str):\n",
    "        totals_ptype = pokemon[(pokemon['Type 1']==ptype) | (pokemon['Type 2']==ptype)]['Total']\n",
    "        pokemon_totals.append(totals_ptype)\n",
    "\n",
    "# Check number of types\n",
    "len(pokemon_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we run the ANOVA test on `pokemon_totals`.\n",
    "- To conduct ANOVA, you can use `scipy.stats.f_oneway()`. Here's the [reference](http://b.link/scipy44).\n",
    "- What if `f_oneway` throws an error because it does not accept `pokemon_totals` as a list? The trick is to add a `*` in front of `pokemon_totals`, e.g. `stats.f_oneway(*pokemon_groups)`. This trick breaks the list and supplies each list item as a parameter for `f_oneway`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The one-way ANOVA tests the null hypothesis that two or \n",
    "# more groups have the same population mean.\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ho: the pokemon types have the same population mean.\n",
    "# Ha: the pokemon types have different populations means. \n",
    "st, p = f_oneway(*pokemon_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null hypothesis rejected\n",
      "Statistic:  6.6175382960055344\n",
      "p-value:  2.6457458815984803e-15\n"
     ]
    }
   ],
   "source": [
    "# Variables \n",
    "alpha = 0.05\n",
    "\n",
    "# Rejection?\n",
    "print(\"Null hypothesis rejected\") if p < alpha else print(\"Null hypothesis can't be rejected\")\n",
    "print('Statistic: ', st)\n",
    "print('p-value: ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret the ANOVA test result. Is the difference significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokemon types have significantly different population means."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
