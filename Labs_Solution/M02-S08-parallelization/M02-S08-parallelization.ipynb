{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ironhack logo](https://i.imgur.com/1QgrNNw.png)\n",
    "\n",
    "# Lab | Parallelization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lab will combine parallelization with some of the other topics you have learned in the Intermediate Python module of this program (list comprehensions, requests library, functional programming, web scraping, etc.). You will write code that extracts a list of links from a web page, requests each URL, and then indexes the page referenced by each link - both sequentially and in parallel.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Multiprocessing Library Documentation](https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#module-multiprocessing)\n",
    "- [Python Parallel Computing (in 60 Seconds or less)](https://dbader.org/blog/python-parallel-computing-in-60-seconds)\n",
    "- [Python Multiprocessing: Pool vs Process â€“ Comparative Analysis](https://www.ellicium.com/python-multiprocessing-pool-process/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQUSp94RAWQ4"
   },
   "source": [
    "## Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:03:36.690119Z",
     "start_time": "2020-06-29T20:03:36.353312Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7yK4Z7r4AWQ4"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:03:37.310177Z",
     "start_time": "2020-06-29T20:03:36.695115Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sAQ_os7vAWQ9"
   },
   "outputs": [],
   "source": [
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8LfHrmwAWRE"
   },
   "source": [
    "## Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:03:37.552021Z",
     "start_time": "2020-06-29T20:03:37.317159Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iZ9gYv9_AWRF"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:03:37.694061Z",
     "start_time": "2020-06-29T20:03:37.556126Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8W-rZeCvAWRI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "link_tags = soup.find_all('a', href=True)\n",
    "links = list(set([link['href'] for link in link_tags]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cX7Ysz7AWRL"
   },
   "source": [
    "## Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "Create a list with the absolute link and remove any that contain a percentage sign (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:03:37.731043Z",
     "start_time": "2020-06-29T20:03:37.709058Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "E3WgzEvTAWRQ"
   },
   "outputs": [],
   "source": [
    "absolute_links = [link for link in links if link.startswith('http') and '%' not in link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGyDklfYAWRk"
   },
   "source": [
    "## Step 4: Write a function called crawl_page that accepts a link and does the following.\n",
    "\n",
    "- Request the content of the page referenced by that link.\n",
    "- Create a soup with the request content.\n",
    "- Extract a list of links\n",
    "- Return the count of links in the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:03:37.842978Z",
     "start_time": "2020-06-29T20:03:37.835980Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K8TjKxI-AWRr"
   },
   "outputs": [],
   "source": [
    "def crawl_page(url):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    html = requests.get(url).content\n",
    "    soup = BeautifulSoup(html)\n",
    "    link_tags = soup.find_all('a', href=True)\n",
    "    return len(link_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOVjV8KWAWRu"
   },
   "source": [
    "## Step 5: Sequentially loop through the list of links, running the crawl_page function each time and save result in a list.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:11:28.102014Z",
     "start_time": "2020-06-29T20:03:37.844975Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JKIj4pJgAWRu",
    "outputId": "8ef05761-b75e-4bb5-ed34-92bb54cea04a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99, 0, 12, 171, 361, 125, 171, 271, 70, 0, 116, 315, 1, 274, 229, 298, 0, 69, 21, 4, 5, 0, 151, 130, 75, 67, 366, 2, 161, 161, 172, 111, 71, 369, 219, 1, 4, 80, 100, 115, 157, 484, 39, 263, 174, 83, 112, 273, 0, 258, 27, 280, 140, 272, 146, 100, 233, 119]\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_list = []\n",
    "for url in absolute_links:\n",
    "    result_list.append(crawl_page(url))\n",
    "\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRo4XekYAWR0"
   },
   "source": [
    "## Step 6: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:11:28.163980Z",
     "start_time": "2020-06-29T20:11:28.104015Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "38khI1NqAWR2"
   },
   "outputs": [],
   "source": [
    "#import multiprocessing\n",
    "import multiprocess\n",
    "# If you are using MaC use the multiprocessing library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:12:38.398687Z",
     "start_time": "2020-06-29T20:11:28.166978Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "T1-dvNOfAWR5",
    "outputId": "c91211cd-154a-45cb-81a1-d8b1ba43e037",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pool = multiprocessing.Pool()\n",
    "pool = multiprocess.Pool()\n",
    "result = pool.map(crawl_page, absolute_links)\n",
    "pool.terminate()\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "paralelization_as.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
